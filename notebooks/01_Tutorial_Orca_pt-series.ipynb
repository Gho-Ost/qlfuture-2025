{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orca Photonic Device Tutorial\n",
    "This notebook will guide you through the basics of understanding and playing with photonic circuits using the Orca device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, perform the relevant imports and navigate to the root folder\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if os.getcwd()[-9:]==\"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from ptseries.tbi import create_tbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ptseries*** is a library created by Orca Computing specifically to work with the Orca photonic divices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define a Photonic Circuit\n",
    "A photonic circuit allows for the manipulation of photons at a microscopic scale for various computational and experimental applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Photonic circuit\n",
    "tbi = create_tbi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Change input state and draw the circuit\n",
    "Define your own input state and draw the circuit using the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_state = ...\n",
    "tbi.draw(input_state=input_state, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Run the circuit\n",
    "In the previous cell you already created the circuit with some number of loops so now we want to run the circuit. To do that we need to set theta angles (in radians). The number of thetas is dependent on the number of loops and input size. It can be calculated as:\n",
    "\n",
    "*<h5><center> (input_size - 1) * n_loops </center></h5>*\n",
    "\n",
    "Also you have to set up how many times you want to run the circuit (parameter called \"n_samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters\n",
    "theta_list = ...\n",
    "n_samples = ...\n",
    "\n",
    "# Draw samples\n",
    "samples = tbi.sample(\n",
    "    input_state=input_state,\n",
    "    theta_list=theta_list,\n",
    "    n_samples=n_samples\n",
    "    )\n",
    "\n",
    "# Plot a histogram of the counts\n",
    "samples_sorted = dict(sorted(samples.items(), key=lambda state: state[0]))\n",
    "labels = list(samples_sorted.keys())\n",
    "labels = [str(i) for i in labels]\n",
    "values = list(samples_sorted.values())\n",
    "plt.bar(range(len(values)), values, tick_label=labels)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot represents how many times we got each of states presented on the x-axis. As you can notice, the sum of values in each state is equal to the sum of ones in the input state but it is impossible to get an output state with all ones.\n",
    "\n",
    "That is happening because of entanglement of photons inside the circuit. \n",
    "\n",
    "You can change the input_state, theta_list and n_samples to check correctness of this property yourself. This should help to understand the logic of the divice better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Change number of loops\n",
    "In the Orca photonic circuits there is a parameter called \"n_loops\". On the drawing you created above you can see the layer of thetas. Those are repesenting angles for rotation of photons. One layer of them called \"a loop\". In function \"create_tbi\" you have a chance to set up the number of loops. Try to do that and draw the circuit to see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbi = create_tbi()\n",
    "tbi.draw(input_state=input_state, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the new circuit. Remember that number of thetas has to be greater. You can use the formula above to calculate it. Also because of increasing number of thetas the circuit will take longer to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters\n",
    "theta_list = ...\n",
    "n_samples = ...\n",
    "\n",
    "# Draw samples\n",
    "samples = tbi.sample(\n",
    "    input_state=input_state,\n",
    "    theta_list=theta_list,\n",
    "    n_samples=n_samples\n",
    "    )\n",
    "\n",
    "# Plot a histogram of the counts\n",
    "samples_sorted = dict(sorted(samples.items(), key=lambda state: state[0]))\n",
    "labels = list(samples_sorted.keys())\n",
    "labels = [str(i) for i in labels]\n",
    "values = list(samples_sorted.values())\n",
    "plt.bar(range(len(values)), values, tick_label=labels)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congrats! Now you know the basics! So we can move to harder tasks such as solving MaxCut problem using Binary Bosonic Solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solving Portfolio Optimization Problem with Binary Bosonic Solver\n",
    "This part of the tutorial will guide you through understanding of the algorithm for solving optimization problems and show the principal of work using an instance of one the famous optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Portfolio Optimization problem is a fundamental task in financial management. It involves selecting the proportion of various assets in a portfolio to maximize returns while minimizing risk, typically measured as the variance or volatility of returns. The challenge is to find the optimal balance between expected return and risk, adhering to constraints such as budget limits and risk tolerance. This problem is NP-hard, making it computationally challenging to solve exactly, especially as the number of assets increases.\n",
    "\n",
    "Let's start by generating a random set of assets with given expected returns and covariances. Then, we will use the Binary Bosonic Solver to find the maximum cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will import all necessary libraries and functions\n",
    "import itertools \n",
    "import random\n",
    "\n",
    "from ptseries.algorithms.binary_solvers.binary_bosonic_solver import BinaryBosonicSolver\n",
    "from ptseries.common.logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can generate random instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for covariance matrix\n",
    "def generate_covariance_matrix(n_assets):\n",
    "    random_matrix = np.random.randn(n_assets,n_assets)\n",
    "    cov_matrix = np.dot(random_matrix,random_matrix.T) # semi definite matrix\n",
    "    return cov_matrix\n",
    "\n",
    "# Function for expected returns\n",
    "def generate_expected_returns(n_assets):\n",
    "    return np.random.randn(n_assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use real data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stock price data as a NumPy array\n",
    "stock_prices = np.array([\n",
    "    [93.043,  51.826, 1.063],\n",
    "    [84.585,  52.823, 0.938],\n",
    "    [111.453, 56.477, 1.000],\n",
    "    [99.525,  49.805, 0.938],\n",
    "    [95.819,  50.287, 1.438],\n",
    "    [114.708, 51.521, 1.700],\n",
    "    [111.515, 51.531, 2.540],\n",
    "    [113.211, 48.664, 2.390],\n",
    "    [104.942, 55.744, 2.130],\n",
    "    [99.827,  47.916, 2.980],\n",
    "    [91.607,  49.438, 1.900],\n",
    "    [107.937, 51.336, 1.750],\n",
    "    [115.590, 55.081, 1.800]\n",
    "])\n",
    "\n",
    "# Columns correspond to: IBM, WMT, SEHI\n",
    "# Rows correspond to months from November-00 to November-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stock returns data as a NumPy array\n",
    "stock_returns = np.array([\n",
    "    [-0.091,  0.019, -0.118],\n",
    "    [0.318,  0.069,  0.067],\n",
    "    [-0.107, -0.118, -0.063],\n",
    "    [-0.037,  0.010,  0.500],\n",
    "    [0.197,  0.025,  0.183],\n",
    "    [-0.028, -0.002,  0.849],\n",
    "    [0.015, -0.056,  0.309],\n",
    "    [0.003,  0.130, -0.107],\n",
    "    [-0.049, -0.142,  0.045],\n",
    "    [-0.082,  0.042, -0.362],\n",
    "    [0.178,  0.038, -0.079],\n",
    "    [0.071,  0.073,  0.029]\n",
    "])\n",
    "\n",
    "# Average returns\n",
    "average_returns = np.array([0.026, 0.008, 0.074])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the covariance matrix as a NumPy array\n",
    "covariance_matrix = np.array([\n",
    "    [0.017087987, 0.003298885, 0.001224849],\n",
    "    [0.003298885, 0.005909044, 0.004488271],\n",
    "    [0.001224849, 0.004488271, 0.063000818]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the visual representation of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming equal weights for simplicity\n",
    "weights = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "# Calculate portfolio return and volatility (standard deviation)\n",
    "portfolio_return = np.dot(stock_returns.mean(axis=0), weights)\n",
    "portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(covariance_matrix, weights)))\n",
    "\n",
    "# Simulate multiple portfolios with random weights\n",
    "n_portfolios = 1000\n",
    "results = np.zeros((n_portfolios, 3))\n",
    "\n",
    "for i in range(n_portfolios):\n",
    "    weights = np.random.random(3)\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    portfolio_return = np.dot(stock_returns.mean(axis=0), weights)\n",
    "    portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(covariance_matrix, weights)))\n",
    "    \n",
    "    sharpe_ratio = portfolio_return / portfolio_volatility\n",
    "    \n",
    "    results[i] = [portfolio_volatility, portfolio_return, sharpe_ratio]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(results[:, 0], results[:, 1], c=results[:, 2], cmap='plasma')\n",
    "plt.colorbar(label='Sharpe Ratio')\n",
    "plt.xlabel('Volatility', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Return', fontsize=12, fontweight='bold')\n",
    "plt.title('Efficient Frontier', fontsize=14, fontweight='bold')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we have an instance we will solve, which means find the most optimal investments with the smallest risks. We will use a binary bosonic solver. A binary bosonic solver (BBS) is a computational tool used to solve optimization problems by modeling them with bosonic particles, which are particles that follow Bose-Einstein statistics. In this context, the problem is represented in a binary form, where variables take on values of 0 or 1. The bosonic solver leverages quantum mechanical principles, particularly the behavior of bosons, to explore the solution space. By simulating the interactions and energy states of these particles, the solver aims to find the optimal configuration that minimizes or maximizes a given cost function. This approach is especially relevant in quantum computing and advanced optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function with application of the BBS\n",
    "def BBS(state, qubo_function, tbi_params={\"tbi_type\": \"single-loop\"}, gradient_mode = \"spsa\"):\n",
    "    problem_size = len(state)\n",
    "    logger = Logger(log_dir=None)\n",
    "    \n",
    "    bbs = BinaryBosonicSolver(\n",
    "        problem_size,\n",
    "        qubo_function,\n",
    "        tbi_params=tbi_params,\n",
    "        gradient_mode=gradient_mode\n",
    "    )\n",
    "\n",
    "    bbs.train(\n",
    "        learning_rate=1e-2,\n",
    "        updates=160,\n",
    "        print_frequency=20,\n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    best_sol, best_energy = bbs.return_solution()\n",
    "\n",
    "    return best_sol, best_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the BBS we will have to create a cost function. To do that we will transform the problem instance into a QUBO matrix. A QUBO (Quadratic Unconstrained Binary Optimization) matrix is a mathematical representation used in optimization problems where the goal is to minimize or maximize a quadratic objective function of binary variables. The matrix encodes the coefficients of the quadratic and linear terms, making it suitable for solving problems using methods like quantum annealing and classical optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating a QUBO matrix\n",
    "def portoflio_qubo_matrix(num_assets, covariance_matrix=None, expected_returns=None, risk=0.5):\n",
    "    if expected_returns is None:\n",
    "        expected_returns = np.random.uniform(low=1, high=10, size=num_assets)\n",
    "    \n",
    "    if covariance_matrix is None:\n",
    "        random_matrix = np.random.uniform(low=0, high=1, size=(num_assets, num_assets))\n",
    "        covariance_matrix = np.dot(random_matrix, random_matrix.T)\n",
    "\n",
    "    Q = np.zeros((num_assets, num_assets))\n",
    "    for i in range(num_assets):\n",
    "        Q[i, i] = -expected_returns[i] + risk * covariance_matrix[i, i]\n",
    "        for j in range(i + 1, num_assets):\n",
    "            Q[i, j] = 2 * risk * covariance_matrix[i, j]\n",
    "            Q[j, i] = Q[i, j]\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Create a Cost Function\n",
    "The next step is to create a cost function based on the QUBO matrix. The cost function evaluates the \"cost\" or \"energy\" associated with a binary vector in the context of the optimization problem. The QUBO matrix \\( Q \\) defines the relationship between the variables, and the cost function computes the value by applying this matrix to the binary vector.\n",
    "\n",
    "To achieve this, the cost function should accept a binary vector as input and calculate its cost using the quadratic form:\n",
    "\n",
    "Cost = bin_vec^T * Q * bin_vec\n",
    "\n",
    "In Python, this can be implemented using the `np.dot` function, which performs the necessary matrix multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = ...\n",
    "\n",
    "def cost_function(bin_vec):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Apply BBS to the Instance\n",
    "The last step is to solve the problem. To do that just get the solution and energy from BBS. Also you can play around with tbi_parameters, gradient_mode and other parameters inside BBS function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = ...\n",
    "solution, energy = BBS(...)\n",
    "print(solution, energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congrats! Now you know how Binary Bosonic Solver works! So we can move to harder tasks such as hybrid neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Hybrid Neural Network\n",
    "This part of the tutorial will guide you through understanding both classical and quantum neural networks and how they can be integrated to form hybrid systems.\n",
    "\n",
    "#### What are Neural Networks?\n",
    "Neural networks are computational models that mimic the human brain's structure and function, using layers of nodes or neurons to process data.\n",
    "\n",
    "#### How do Neural Networks Work?\n",
    "A neural network processes input data through layers to make predictions. It learns by adjusting its neurons' weights based on the errors in its predictions.\n",
    "\n",
    "A feedforward neural network is one of the simplest types of artificial neural networks devised. In this network, the information moves in only one direction—forward—from the input nodes, through the hidden nodes (if any), and to the output nodes. There are no cycles or loops in the network. Feedforward neural networks were the first type of artificial neural network invented and are simpler than their counterparts like recurrent neural networks and convolutional neural networks. You can read more here: https://deepai.org/machine-learning-glossary-and-terms/feed-forward-neural-network\n",
    "\n",
    "Any neural network contains the following parts:\n",
    "1. Input Layer\n",
    "2. Hidden Layers\n",
    "3. Output Layer\n",
    "4. Activation Function\n",
    "5. Loss Function\n",
    "6. Optimizer\n",
    "\n",
    "Firstly we need to understand what is the layer of neural network. There are several types or configurations of layers but easiest of them is linear layers so we will take a look at its structure. You can about other types of layers here: https://pytorch.org/docs/stable/nn.html#convolution-layers\n",
    "\n",
    "Each linear layer consists of n number of neurons. For example number of neurons in input layer is equal to the number of inputs. In MNIST dataset (famous image dataset) size of images is 28*28 so the number of neurons will be 784. Output layer for MNIST consists of 10 neurons since the number of digits (so the number of classes for classification as well) equals to 10 (from 0 to 9). In hidden layers amount of neurons can be different.\n",
    "\n",
    "Each neuron have activation function. The activation function is a formula which changes the input number. There are many different activation functions such as sigmoid, tanh, softmax, relu, etc. You can read more about them here: https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
    "\n",
    "After we chose amount of layers and number of neurons for each layer, they will be automatically connected to each other (each neuron from a layer is connected to each neuron of the next layer). The stucture of a neural networks looks like this:\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1358/1*3fA77_mLNiJTSgZFhYnU0Q.png\" width=550 height=350>\n",
    "\n",
    "Each of this connection has *weight* which is a number that is usigng in the general formula for neural networks. This formula is:\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/neural_networks_5.png\" width=600 height=230>\n",
    "\n",
    "Where x_i is *output* from each neuron from previous layer, w_i is *weight* of each connection, b is bias (which is just a constant value) and z is the number that goes to a neuron in next layer.\n",
    "\n",
    "\n",
    "After creating a class with configuration of the network we have to choose loss function and the optimizer. They will be used in training and evaluation functions.\n",
    "\n",
    "The loss function is a method of evaluating how well your machine learning algorithm models your featured data set. In other words, loss functions are a measurement of how good your model is in terms of predicting the expected outcome.\n",
    "\n",
    "Optimizer ties together the loss function and model parameters by updating the model in response to the output of the loss function. In simpler terms, optimizers shape and mold your model into its most accurate possible form by futzing with the weights. The loss function is the guide to the terrain, telling the optimizer when it’s moving in the right or wrong direction.\n",
    "\n",
    "The training function is the overall algorithm that is used to train the neural network to recognize a certain input and map it to an output. Inside the function you are computing loss function and depending on it update weights.\n",
    "\n",
    "The evaluation function is used to evaluate the performance of the trained model. In other words, the evaluation function is used to measure how well your model is performing on new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move to the coding part. First we import all needed libraries including ptseries and PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, perform the relevant imports and navigate to the root folder\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "if os.getcwd()[-9:]==\"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from ptseries.models.utils import calculate_n_params, calculate_output_dim\n",
    "from ptseries.models.pt_layer import PTLayer\n",
    "from ptseries.algorithms.classifiers import VariationalClassifier\n",
    "from ptseries.algorithms.classifiers.utils import create_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***PTLayer*** is a quantum layer for a neural network. It was built to work with PyTorch so it will be easy to integrate it to classical neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a hybrid neural network for binary classification. So first of all we will create a dataset for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 4*torch.rand((200,2))-2\n",
    "labels = torch.where(data[:,0]**2+data[:,1]**2 < 2, 1, 0).unsqueeze(1).to(torch.float32)\n",
    "plt.scatter(data[:,0], data[:,1], c=labels)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the plot you can see the training data. So the next step we will set a train dataloader and its batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(data, labels, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create a hybrid neural network using PyTorch and PTLayer.\n",
    "\n",
    "### Task 1: Set the size of inputs\n",
    "\n",
    "The size of input size is determined by the dataset object size. In the code below you should set the parameter \"self.input_size\" as the size of the dataset object. \n",
    "\n",
    "### Task 2: Play with the neural network structure\n",
    "\n",
    "You can modify the neural network structure by adding more layers, changing the number of neurons, using different activation functions, etc. The current structure is described in \"self.net\" parameter in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, tbi_params=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = ...\n",
    "        self.input_state = (0, 1, 0)\n",
    "        self.tbi_params = tbi_params\n",
    "        self.observable = 'avg-photons'\n",
    "\n",
    "        # we use calculate_n_params to determine the number of beam splitter angles\n",
    "        n_params = calculate_n_params(self.input_state, tbi_params=self.tbi_params)\n",
    "        n_outputs = calculate_output_dim(\n",
    "            self.input_state,\n",
    "            tbi_params=self.tbi_params,\n",
    "            observable=self.observable\n",
    "            )\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, n_params),\n",
    "            PTLayer(\n",
    "                self.input_state,\n",
    "                in_features=n_params,\n",
    "                observable=self.observable,\n",
    "                tbi_params=self.tbi_params,\n",
    "                n_samples=200\n",
    "            ),\n",
    "            nn.Linear(n_outputs, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for training! Before training we will print model to make sure everything is right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining loss function. You can choose your own loss function which can give better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "classifier = VariationalClassifier(model, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Set up training parameters\n",
    "To make training of the neural network. You should set up your learning rate and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(\n",
    "    train_dataloader,        # pytorch dataloader containing the training data\n",
    "    learning_rate=...,       # very small number \n",
    "    epochs=...,              # number of training iterations\n",
    "    print_frequency=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the results of the training. We can compare graphs of the training data and the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some new test data and perform inference\n",
    "data_test = 4*torch.rand((200,2))-2\n",
    "predictions = classifier.forward(data_test)\n",
    "\n",
    "# Convert predictions to blue/yellow binary labels and plot the result\n",
    "binarized_predictions = torch.where(predictions < 0.5, 0, 1).unsqueeze(1)\n",
    "plt.scatter(data_test[:,0], data_test[:,1], c=binarized_predictions)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we will check the accuracy of the NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "for _, labels in train_dataloader:\n",
    "    true_labels.extend(labels.numpy())\n",
    "\n",
    "# Convert true labels to a tensor\n",
    "true_labels = torch.tensor(true_labels)\n",
    "\n",
    "# Assuming binary classification, round predictions to 0 or 1\n",
    "predicted_labels = torch.round(predictions)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = torch.mean((predicted_labels == true_labels).float())\n",
    "\n",
    "print(f'Accuracy: {accuracy.item() * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congrats! Now you know what are hybrid neural networks and how to create and train them! So we can move to the real hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real Quantum Hardware Use\n",
    "\n",
    "Now that you have trained your hybrid quantum-classical neural network, you can use it to make predictions on real quantum hardware.\n",
    "\n",
    "Here is an example of how to use the real Orca computer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A connection to the device must be opened. We can do this by calling the `create_tbi` function with the `tbi_type` set to PT-1. Note that the object has some differences to the simulated backends you may have used previously. Specifically, you cannot specify parameters such as beam splitter loss becasue this is now a property of the device. For a full comparison, please consult the SDK documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bin_interferometer = create_tbi(\n",
    "    tbi_type=\"PT-1\",\n",
    "    ip_address=\"192.168.34.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object has methods that allow us to check the connection between the computer and the PT-1. We can also check the status of the hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Solving MaxCut using BBS on real Orca computer\n",
    "\n",
    "Solve MaxCut problem using BBS. Change tbi_params in the BBS to use the real Orca device.\n",
    "\n",
    "***Important! The maximum numer of parameters (thetas) that is availible is 5 so make the instances of MaxCut with maximum 6 vertices***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, suppose that we have a set of different computers, each with different types of connections. Some computers have bluetooth, some have USB ports, HDMI ports, etc. We want to split our set of computers into two groups for two different projects, but it's very important that the two groups can connect to each other. The problem is sometimes the wires and connections don't work! How can we be sure that we have the best chance at remaining connected?\n",
    "\n",
    "One way to solve this problem is with the maximum cut problem. If we think of our set of computers as a graph (a node/vertex for each computer), and draw an edge between computers that can connect to each other, we have a model of our network. If we look for a maximum cut in our graph, then we are looking for a way to split the nodes into two groups so that there are as many edges as possible between the groups. In our computer set, this means that we have two groups with as many connections as possible between the two groups. Now if one connection goes down, we have many more to use! This way we have created a more resilient network by providing many redundant connections between groups in case one connection fails.\n",
    "\n",
    "Below we see a simple network of five nodes and three different ways to split the set of nodes into two groups. The dashed edges in each graph highlight the cut edges.\n",
    "\n",
    "<img src=\"https://github.com/dwave-examples/maximum-cut/raw/master/readme_imgs/cut_examples.png\">\n",
    "\n",
    "Then we have to transform this problem to QUBO matrix so the BBS can solve it. The objective function that we are looking to optimize is maximizing the number of cut edges. To count how many cut edges we have given a partition of the nodes (assignment of our binary variables), we consider a single edge in a graph in the table below. We only want to count an edge if the endpoints are in different subsets, and so we assign a 1 for the edge_score column in this case and a 0 otherwise.\n",
    "| $x_i$ | $x_j$ | edge\\_score $(i,j)$ |\n",
    "|:-----:|:-----:|:-------------------:|\n",
    "|   0   |   0   |          0           |\n",
    "|   0   |   1   |          1           |\n",
    "|   1   |   0   |          1           |\n",
    "|   1   |   1   |          0           |\n",
    "\n",
    "From this table, we see that we can use the expression x_i+x_j-2x_ix_j to calculate the edge_score column in our table. Now for our entire graph, our objective function can be written as shown below, where the sum is over all edges in the graph.\n",
    "\n",
    "<img src=\"https://github.com/dwave-examples/maximum-cut/raw/master/readme_imgs/QUBO.png\">\n",
    "\n",
    "Since our system is used to minimize an objective function, we must convert this maximization problem to a minimization problem by multiplying the expression by -1. Our final QUBO expression is the following.\n",
    "\n",
    "<img src=\"https://github.com/dwave-examples/maximum-cut/raw/master/readme_imgs/final_QUBO.png\">\n",
    "\n",
    "Now we can use the BBS to solve our maximum cut problem. First step is to create an instance of MaxCut, then convert it to QUBO and solve using BBS. Then you can try to use the actual hardware to solve the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can copy some code here and modify it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Hybrid neural network with the real Orca computer\n",
    "\n",
    "Change tbi_params in the hybrid neural network above so that it will train with the real Orca device. To make the task more complex you can change number of loops in the tbi_params but then also number of thetas will increase which means it must be calculated inside the HNN.\n",
    "\n",
    "***Important! The maximum numer of parameters (thetas) that is availible is 5 so make the input objects with maximum size equal to 6***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can copy neural network here or create a new one with and change tbi_params in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congrats! You finished the tutorial! Now you know how to use real quantum device and apply it to neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
